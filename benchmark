#!/usr/bin/env python3
import os
import re
import csv
import sys
import subprocess
import argparse



# Define your commands
commands = {
    "HF-TRT": "python3 $MODEL/convert_checkpoint.py --model_dir $MODEL_PATH/ --dtype float16 --output_dir $MODEL_DIR/trt_ckpt/",
    "HF-TRT-Quantized": "python3 $MODEL/convert_checkpoint.py --model_dir $MODEL_PATH/ --dtype float16 --use_weight_only --output_dir $MODEL_DIR/trt_ckpt/quantized/",
    "TRT-engine": "trtllm-build --checkpoint_dir $MODEL_DIR/trt_ckpt/ --gemm_plugin float16 --output_dir $MODEL_DIR/trt_engines/",
    "TRT-engine-Quantized": "trtllm-build --checkpoint_dir $MODEL_DIR/trt_ckpt/quantized/ --gemm_plugin float16 --output_dir $MODEL_DIR/trt_engines/quantized/"
}

commands_benchmark = {
    "Hugging Face": "python3 $MODEL/../summarize.py --test_hf --hf_model_dir $MODEL_PATH/ --data_type fp16 --engine_dir $MODEL_DIR/trt_engines",
    "TensorRT-LLM": "python3 $MODEL/../summarize.py --test_trt_llm --hf_model_dir $MODEL_PATH/ --data_type fp16 --engine_dir $MODEL_DIR/trt_engines/",
    "TensorRT-LLM (INT)": "python3 $MODEL/../summarize.py --test_trt_llm --hf_model_dir $MODEL_PATH/ --data_type fp16 --engine_dir $MODEL_DIR/trt_engines/quantized/"
}

# Choose the model you want to benchmark
# Argmunets from CLI : link to huggingface model


def convert_model_to_trt():
    keys = "HF-TRT"
    run_command(commands[keys])
    print("==================== Model converted to tensor checkpoint ====================")
    print()


def convert_model_to_trt_and_quantify():
    keys = "HF-TRT-Quantized"
    run_command(commands[keys])
    print("==================== Model converted to tensor checkpoint and quantized to INT ====================")
    print()


def build_engine():
    keys = "TRT-engine"
    run_command(commands[keys])
    print("==================== Engine create ====================")
    print()

def build_engine_quantized():
    keys = "TRT-engine-Quantized"
    run_command(commands[keys])
    print("==================== Engine create for quantized tensor ====================")
    print()


def run_command(command):
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    return result.stdout

def parse_output(output):
    # Extracting execution time
    time_match = re.search(r'real\s+(\d+)m([\d.]+)s', output)
    if time_match:
        minutes = int(time_match.group(1))
        seconds = float(time_match.group(2))
        exec_time = minutes * 60 + seconds
    else:
        exec_time = None
    
    rouge_scores = re.findall(r'rouge[12Lsum]+ : ([\d.]+)', output)
    rouge_scores = [float(score) for score in rouge_scores] if rouge_scores else []
    
    latency_match = re.search(r'total latency: ([\d.]+) sec', output)
    latency = float(latency_match.group(1)) if latency_match else None
    
    tokens_match = re.search(r'total output tokens: (\d+)', output)
    total_tokens = int(tokens_match.group(1)) if tokens_match else None
    
    tokens_per_sec_match = re.search(r'tokens per second: ([\d.]+)', output)
    tokens_per_sec = float(tokens_per_sec_match.group(1)) if tokens_per_sec_match else None
    
    return exec_time, rouge_scores, latency, total_tokens, tokens_per_sec

def benchmark_model(quantify : bool):

    print("==================== Benchmark in progress ====================")
    print()
    # Run commands and parse outputs
    results = {}
    for key, command in commands_benchmark.items():
        if quantify:
            output = run_command(command)
            print(output)
            results[key] = parse_output(output)
        else: 
            if key != "TensorRT-LLM (INT)":
                output = run_command(command)
                print(output)
                results[key] = parse_output(output)

    # Prepare CSV data
    data = [
        ["Model", "Exec Time (s)", "ROUGE-1", "ROUGE-2", "ROUGE-L", "Latency (s)", "Total Tokens", "Tokens/s"],
        ["Hugging Face", results["Hugging Face"][0]] + results["Hugging Face"][1] + [None, None, None],
        ["TensorRT-LLM", results["TensorRT-LLM"][0]] + results["TensorRT-LLM"][1] + [results["TensorRT-LLM"][2], results["TensorRT-LLM"][3], results["TensorRT-LLM"][4]]
    ]

    if quantify:
        data.append(["TensorRT-LLM (INT)", results["TensorRT-LLM (INT)"][0]] + results["TensorRT-LLM (INT)"][1] + [results["TensorRT-LLM (INT)"][2], results["TensorRT-LLM (INT)"][3], results["TensorRT-LLM (INT)"][4]])

    # Write to CSV
    with open("/TensorRT-LLM/output.csv", "w", newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(data)

    print("==================== Benchmark done results saved at /TensorRT-LLM/output.csv ====================")



def helper():
    print(
    """
        Usage: benchmark.py -m <model name> -c <clone model name>
        Example: benchmark.py -m bloom -c bloom-560m

        -h    --help 
        -m    --model  
        -b    --benchmark
        -c    --huggingface
        -q    --quantization


        -m get the name of the tested model 
        -b start benchmark only if engine is already created 
        -c get the name of the hugging face model once it is clone
        -q quantification of the model to INT4 or INT8
        
    """
    )

def main():

    quantification = False

    if len(sys.argv) <2:
        helper()
        sys.exit(0)
    if sys.argv[1] == "help":
        helper()
        sys.exit(0)
    
    # Add arguments
     # Create the parser
    parser = argparse.ArgumentParser(description='Benchmark models with optional flags.', add_help=False)
    
    # Add arguments
    parser.add_argument('-h', '--help', action='store_true', help='Show this help message and exit')
    parser.add_argument('-m', '--model', type=str, help='Name of the model to benchmark', required=True)
    parser.add_argument('-b', '--benchmark', action='store_true', help='Benchmark only when engines are already created')
    parser.add_argument('-c', '--huggingface', type=str, help='Name of the huggingface model', required=True)
    parser.add_argument('-q', '--quantification', action='store_true', help='Quantification of the model to INT4 or INT8')

    args = parser.parse_args()

    # Check for help 
    if args.help:
        parser.print_help()
        print("If you want more explanation about the arguments, please run './benchmark.py help'")
        sys.exit(0)

    if args.quantification:
        quantification = True


    model_name = args.model
    model_hf = args.huggingface
    MODEL_PATH = f"../model/{model_hf}"
    MODEL = f"../examples/{model_name}"
    MODEL_DIR = f"../model"
    print(f"Model path: {MODEL_PATH}")
    print(f"Model: {MODEL}")
    print(f"Model dir: {MODEL_DIR}")
    os.environ['MODEL_PATH'] = MODEL_PATH
    os.environ['MODEL'] = MODEL
    os.environ['MODEL_DIR'] = MODEL_DIR
    
    if args.benchmark:
        # Perform benchmarking only
        benchmark_model(quantification)
    else:
        # Full process
        convert_model_to_trt()
        build_engine()
        if quantification == True:
            convert_model_to_trt_and_quantify()
            build_engine_quantized()
        
        benchmark_model(quantification)

if __name__ == "__main__":
    main()
